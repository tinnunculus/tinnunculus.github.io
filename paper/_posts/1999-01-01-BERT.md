---
layout: post
title: BERT
sitemap: false
---

**참고**  
[1] <https://arxiv.org/pdf/1810.04805.pdf>  
* * *  

<p align="center"><img width="650" src="/assets/img/paper/bert/2.png"></p>

## Abstract
* BERT는 Bidrectional Encoder Representations from Transformers
* BERT는 labeled 되어 있지 않은 두 문장의 bidirectional representation들을 알아내도록 pre-train 된다.
* 그렇게 pretrained BERT 모델은 마지막단에 layer를 추가함으로써 다양한 Task에 finetuned 시킬 수 있다.
* 다양한 Task로는 question answering, language inference 등이 있다. Task에 Specific한 모델 필요 없이 pretrained BERT에 추가적인 Layer으로만 학습할 수 있다.
*