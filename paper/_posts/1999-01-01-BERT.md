---
layout: post
title: BERT
sitemap: false
---

**참고**  
[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for
Language Understanding, 2018  
* * *  

<p align="center"><img width="550" src="/assets/img/paper/HIERARCHICAL_MULTI-SCALE_ATTENTION_FOR_SEMANTIC_SEGMENTATION/3.png"></p>

## Abstract
* BERT는 Bidrectional Encoder Representations from Transformers
* BERT는 labeled 되어 있지 않은 두 문장의 bidirectional representation들을 알아내도록 pre-train 된다.
* 그렇게 pretrained BERT 모델은 마지막단에 layer를 추가함으로써 다양한 Task에 finetuned 시킬 수 있다.
* 다양한 Task로는 question answering, language inference 등이 있다. Task에 Specific한 모델 필요 없이 pretrained BERT에 추가적인 Layer으로만 학습할 수 있다.
*